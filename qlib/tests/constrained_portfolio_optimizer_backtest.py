#!/usr/bin/env python3
"""
ConstrainedPortfolioOptimizerå›æµ‹æµ‹è¯•
"""

import os
import sys
import warnings
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from qlib.contrib.strategy.optimizer.constrained_optimizer import ConstrainedPortfolioOptimizer

# å…¨å±€èµ„é‡‘è´¹ç‡æ•°æ®ç¼“å­˜
_funding_rates_cache = None
_funding_rates_data = None

# ä½¿ç”¨æ ‡å‡†loggingï¼Œé¿å…è¯¦ç»†æ§åˆ¶å°è¾“å‡º
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def load_funding_rates_data(funding_rates_path: str = r"D:\temp\å¸¦æ—¶é—´é—´éš”çš„å†å²èµ„é‡‘è´¹ç‡æ•°æ®-ä¿®å¤ç¼–ç -all_data.parquet") -> pd.DataFrame:
    """
    åŠ è½½èµ„é‡‘è´¹ç‡æ•°æ®
    
    Parameters
    ----------
    funding_rates_path : str
        èµ„é‡‘è´¹ç‡æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns
    -------
    pd.DataFrame
        MultiIndexæ ¼å¼çš„èµ„é‡‘è´¹ç‡æ•°æ®ï¼Œindex=['datetime', 'symbol'], columns=['funding_rate_interval', 'funding_rate']
    """
    global _funding_rates_data
    
    if _funding_rates_data is not None:
        return _funding_rates_data
    
    try:
        logger.info(f"åŠ è½½èµ„é‡‘è´¹ç‡æ•°æ®: {funding_rates_path}")
        _funding_rates_data = pd.read_parquet(funding_rates_path)
        
        # è½¬æ¢èµ„é‡‘è´¹ç‡ä¸ºæ•°å€¼ç±»å‹ï¼ˆå»é™¤ç™¾åˆ†å·ï¼‰
        if _funding_rates_data['funding_rate'].dtype == 'object':
            _funding_rates_data['funding_rate'] = _funding_rates_data['funding_rate'].str.rstrip('%').astype(float) / 100
        
        logger.info(f"èµ„é‡‘è´¹ç‡æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {_funding_rates_data.shape}")
        return _funding_rates_data
        
    except Exception as e:
        logger.error(f"åŠ è½½èµ„é‡‘è´¹ç‡æ•°æ®å¤±è´¥: {e}")
        raise


def get_funding_rates_for_timestamp(funding_rates_df: pd.DataFrame, timestamp: pd.Timestamp, symbols: list, use_case: str = 'optimizer') -> pd.Series:
    """
    è·å–æŒ‡å®šæ—¶é—´æˆªé¢å’Œå“ç§åˆ—è¡¨çš„èµ„é‡‘è´¹ç‡æ•°æ®
    
    Parameters
    ----------
    funding_rates_df : pd.DataFrame
        èµ„é‡‘è´¹ç‡æ•°æ®
    timestamp : pd.Timestamp
        æ—¶é—´æˆ³
    symbols : list
        å“ç§åˆ—è¡¨
    use_case : str, optional
        ä½¿ç”¨åœºæ™¯: 
        - 'optimizer': ç”¨äºä¼˜åŒ–å™¨èµ„é‡‘è´¹ç‡çº¦æŸï¼ˆé»˜è®¤ï¼‰ï¼Œåªè¿”å›funding_rate_intervalä¸º'1å°æ—¶'çš„å“ç§èµ„é‡‘è´¹ç‡
        - 'portfolio': ç”¨äºç»„åˆèµ„é‡‘è´¹ç‡è®¡ç®—ï¼Œè¿”å›æ‰€æœ‰å“ç§çš„èµ„é‡‘è´¹ç‡
        
    Returns
    -------
    pd.Series
        è¯¥æ—¶é—´æˆªé¢çš„èµ„é‡‘è´¹ç‡æ•°æ®ï¼Œç¼ºå¤±çš„å“ç§èµ„é‡‘è´¹ç‡ä¸º0
    """
    try:
        # è·å–è¯¥æ—¶é—´æˆªé¢çš„èµ„é‡‘è´¹ç‡æ•°æ®
        current_slice = funding_rates_df.loc[timestamp]
        
        # ä¸ºæŒ‡å®šçš„å“ç§åˆ—è¡¨è·å–èµ„é‡‘è´¹ç‡ï¼Œç¼ºå¤±çš„å“ç§è®¾ä¸º0
        funding_rates = pd.Series(0.0, index=symbols)
        
        # å¤šä¸ªå“ç§çš„æƒ…å†µï¼ˆç§»é™¤å•å“ç§å¤„ç†ï¼Œå› ä¸ºæˆªé¢ä¸­æ€§ç­–ç•¥ä¸å¯èƒ½åªæœ‰ä¸€ä¸ªå“ç§ï¼‰
        available_symbols = current_slice.index.get_level_values(0) if current_slice.index.nlevels > 1 else current_slice.index
        common_symbols = set(available_symbols) & set(symbols)
        
        for symbol in common_symbols:
            if symbol in current_slice.index:
                # æ£€æŸ¥ä½¿ç”¨åœºæ™¯
                if use_case == 'optimizer':
                    # ç”¨äºä¼˜åŒ–å™¨èµ„é‡‘è´¹ç‡çº¦æŸï¼Œåªå¤„ç†funding_rate_intervalä¸º'1å°æ—¶'çš„å“ç§
                    if current_slice.loc[symbol, 'funding_rate_interval'] == '1å°æ—¶':
                        funding_rates.loc[symbol] = current_slice.loc[symbol, 'funding_rate']
                    # å¦åˆ™èµ„é‡‘è´¹ç‡è®¾ä¸º0ï¼Œä¸å½±å“ä¼˜åŒ–å™¨è®¡ç®—
                else:
                    # ç”¨äºç»„åˆèµ„é‡‘è´¹ç‡è®¡ç®—ï¼Œè¿”å›æ‰€æœ‰å“ç§çš„èµ„é‡‘è´¹ç‡
                    funding_rates.loc[symbol] = current_slice.loc[symbol, 'funding_rate']
        
        return funding_rates
        
    except KeyError:
        # è¯¥æ—¶é—´æˆªé¢æ²¡æœ‰èµ„é‡‘è´¹ç‡æ•°æ®ï¼Œæ‰€æœ‰å“ç§èµ„é‡‘è´¹ç‡ä¸º0
        return pd.Series(0.0, index=symbols)
        
    except Exception as e:
        logger.error(f"è·å–æ—¶é—´æˆªé¢ {timestamp} çš„èµ„é‡‘è´¹ç‡æ•°æ®å¤±è´¥: {e}")
        raise

# æŠ‘åˆ¶æ§åˆ¶å°è¾“å‡ºï¼Œåªä¿ç•™è¿›åº¦ä¿¡æ¯
logging.getLogger().setLevel(logging.WARNING)
logging.getLogger('qlib').setLevel(logging.WARNING)

# å¯¼å…¥è¿›åº¦æ¡
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print("è­¦å‘Š: tqdmæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„printè¿›åº¦æ˜¾ç¤º")

# é…ç½®æ—¥å¿— - æ—¥å¿—æ–‡ä»¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•
test_dir = Path(r"d:\PycharmProjects\qlib\workspace\py\test\log")
test_dir.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
log_file_path = test_dir / f'constrained_optimizer_backtest_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(
            log_file_path,
            encoding='utf-8'
        ),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class BacktestSimulator:
    """å›æµ‹æ¨¡æ‹Ÿå™¨ï¼šæ¨¡æ‹Ÿæˆªé¢ä¸­æ€§ç­–ç•¥çš„è·¯å¾„ä¾èµ–è°ƒä»“è¿‡ç¨‹"""
    
    def __init__(self, 
                 optimizer: ConstrainedPortfolioOptimizer,
                 initial_capital: float = 1000000.0,
                 funding_rates_data: Optional[pd.DataFrame] = None,
                 price_data: Optional[pd.DataFrame] = None,
                 commission_rate: float = 0.0005):
        """
        åˆå§‹åŒ–å›æµ‹æ¨¡æ‹Ÿå™¨
        
        Parameters
        ----------
        optimizer : ConstrainedPortfolioOptimizer
            çº¦æŸæŠ•èµ„ç»„åˆä¼˜åŒ–å™¨
        initial_capital : float
            åˆå§‹èµ„é‡‘
        funding_rates_data : pd.DataFrame, optional
            èµ„é‡‘è´¹ç‡æ•°æ®ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨çœŸå®èµ„é‡‘è´¹ç‡ï¼Œå¦åˆ™ä½¿ç”¨0
        price_data : pd.DataFrame, optional
            ä»·æ ¼æ•°æ®ï¼Œç”¨äºè®¡ç®—æ”¶ç›Šç‡ï¼Œæ ¼å¼ä¸ºMultiIndex[datetime, instrument]ï¼Œcolumns=['open', 'close']
        commission_rate : float, optional
            æ‰‹ç»­è´¹ç‡ï¼Œé»˜è®¤å•è¾¹ä¸‡åˆ†ä¹‹äº”
        """
        self.optimizer = optimizer
        self.initial_capital = initial_capital
        self.current_capital = initial_capital
        self.funding_rates_data = funding_rates_data
        self.price_data = price_data
        self.returns_data = None  # é¢„è®¡ç®—çš„æ”¶ç›Šç‡æ•°æ®
        self.commission_rate = commission_rate  # æ‰‹ç»­è´¹ç‡ï¼Œå•è¾¹ä¸‡åˆ†ä¹‹äº”
        
        # è®°å½•å†å²
        self.portfolio_history = []
        self.trade_history = []
        self.performance_history = []
        self.optimization_status_history = []  # æ–°å¢ï¼šä¼˜åŒ–çŠ¶æ€å†å²è®°å½•
        
        # åˆå§‹åŒ–æŒä»“ï¼ˆç©ºä»“ï¼‰
        self.current_weights = None
        self.current_positions = {}
        
        logger.info(f"å›æµ‹æ¨¡æ‹Ÿå™¨åˆå§‹åŒ–å®Œæˆï¼Œåˆå§‹èµ„é‡‘: {initial_capital:,.2f}")
        if funding_rates_data is not None:
            logger.info(f"å·²åŠ è½½èµ„é‡‘è´¹ç‡æ•°æ®ï¼Œæ•°æ®å½¢çŠ¶: {funding_rates_data.shape}")
        if price_data is not None:
            logger.info(f"å·²åŠ è½½ä»·æ ¼æ•°æ®ï¼Œæ•°æ®å½¢çŠ¶: {price_data.shape}")
            self._calculate_returns_data()
    
    def load_prediction_data(self, data_path: str) -> pd.DataFrame:
        """
        åŠ è½½é¢„æµ‹æ•°æ®
        
        Parameters
        ----------
        data_path : str
            parquetæ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns
        -------
        pd.DataFrame
            MultiIndexé¢„æµ‹æ•°æ®ï¼Œindex=[datetime, instrument]
        """
        logger.info(f"åŠ è½½é¢„æµ‹æ•°æ®: {data_path}")
        
        try:
            pred_df = pd.read_parquet(data_path)
            logger.info(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œæ•°æ®å½¢çŠ¶: {pred_df.shape}")
            logger.info(f"æ—¶é—´èŒƒå›´: {pred_df.index.get_level_values(0).min()} åˆ° {pred_df.index.get_level_values(0).max()}")
            logger.info(f"è‚¡ç¥¨æ•°é‡: {pred_df.index.get_level_values(1).nunique()}")
            
            return pred_df
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _calculate_returns_data(self):
        """
        é¢„è®¡ç®—æ”¶ç›Šç‡æ•°æ®ï¼šåŸºäºopenä»·æ ¼è®¡ç®—ä¸Šä¸€æœŸåˆ°å½“å‰çš„æ”¶ç›Šç‡
        æ”¶ç›Šç‡ = (å½“å‰open - ä¸Šä¸€æœŸopen) / ä¸Šä¸€æœŸopen
        """
        try:
            logger.info("å¼€å§‹é¢„è®¡ç®—æ”¶ç›Šç‡æ•°æ®...")
            
            # æŒ‰å“ç§åˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªå“ç§çš„openä»·æ ¼å˜åŒ–ç‡
            returns_list = []
            
            for symbol in self.price_data.index.get_level_values(1).unique():
                symbol_data = self.price_data.loc[pd.IndexSlice[:, symbol], :]
                symbol_data = symbol_data.sort_index()
                
                # è®¡ç®—openä»·æ ¼çš„å½“æœŸæ”¶ç›Šç‡ï¼ˆå½“å‰åˆ°ä¸‹ä¸€æœŸçš„å˜åŒ–ç‡ï¼‰
                symbol_returns = symbol_data['open'].pct_change().shift(-1)  # ç§»åä¸€æœŸï¼Œç¡®ä¿tå¯¹åº”tåˆ°t+1çš„æ”¶ç›Šç‡
                symbol_returns.name = 'return'
                
                returns_list.append(symbol_returns)
            
            # åˆå¹¶æ‰€æœ‰å“ç§çš„æ”¶ç›Šç‡æ•°æ®
            self.returns_data = pd.concat(returns_list)
            self.returns_data = self.returns_data.sort_index()
            
            logger.info(f"æ”¶ç›Šç‡æ•°æ®è®¡ç®—å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {self.returns_data.shape}")
            logger.info(f"æ”¶ç›Šç‡ç»Ÿè®¡: å‡å€¼={self.returns_data.mean():.6f}, "
                       f"æ ‡å‡†å·®={self.returns_data.std():.6f}, "
                       f"æœ€å°å€¼={self.returns_data.min():.6f}, "
                       f"æœ€å¤§å€¼={self.returns_data.max():.6f}")
            
        except Exception as e:
            logger.error(f"é¢„è®¡ç®—æ”¶ç›Šç‡æ•°æ®å¤±è´¥: {e}")
            raise
    
    def get_returns_for_timestamp(self, timestamp: pd.Timestamp, symbols: list) -> pd.Series:
        """
        è·å–æŒ‡å®šæ—¶é—´æˆªé¢å’Œå“ç§åˆ—è¡¨çš„å½“æœŸæ”¶ç›Šç‡æ•°æ®ï¼ˆå½“å‰openåˆ°ä¸‹ä¸€æœŸopençš„å˜åŒ–ç‡ï¼‰
        
        Parameters
        ----------
        timestamp : pd.Timestamp
            å½“å‰æ—¶é—´æˆ³
        symbols : list
            å“ç§åˆ—è¡¨
            
        Returns
        -------
        pd.Series
            å½“å‰æ—¶é—´æˆªé¢çš„æ”¶ç›Šç‡æ•°æ®ï¼ˆå½“å‰åˆ°ä¸‹ä¸€æœŸçš„å˜åŒ–ç‡ï¼‰ï¼Œç¼ºå¤±çš„å“ç§æ”¶ç›Šç‡ä¸º0
        """
        try:
            if self.returns_data is None:
                logger.warning("æ”¶ç›Šç‡æ•°æ®æœªåŠ è½½ï¼Œè¿”å›0æ”¶ç›Šç‡")
                return pd.Series(0.0, index=symbols)
            
            # è·å–å½“å‰æ—¶é—´æˆªé¢çš„æ”¶ç›Šç‡æ•°æ®ï¼ˆå½“å‰åˆ°ä¸‹ä¸€æœŸçš„å˜åŒ–ç‡ï¼‰
            current_returns = self.returns_data.loc[timestamp]
            
            # ä¸ºæŒ‡å®šçš„å“ç§åˆ—è¡¨è·å–æ”¶ç›Šç‡ï¼Œç¼ºå¤±çš„å“ç§è®¾ä¸º0
            returns = pd.Series(0.0, index=symbols)
            
            # å¤šä¸ªå“ç§çš„æƒ…å†µï¼ˆæ—¶é—´æˆªé¢æ•°æ®ï¼‰
            available_symbols = current_returns.index
            common_symbols = set(available_symbols) & set(symbols)
            
            for symbol in common_symbols:
                returns.loc[symbol] = current_returns.loc[symbol]
            
            logger.debug(f"æ—¶é—´ {timestamp}: æ”¶ç›Šç‡æ•°æ®è·å–å®Œæˆï¼Œå…± {len(symbols)} ä¸ªå“ç§ï¼Œ"
                        f"å…¶ä¸­ {returns[returns != 0].count()} ä¸ªå“ç§æœ‰éé›¶æ”¶ç›Šç‡")
            
            return returns
            
        except KeyError:
            # è¯¥æ—¶é—´æˆªé¢æ²¡æœ‰æ”¶ç›Šç‡æ•°æ®ï¼Œæ‰€æœ‰å“ç§æ”¶ç›Šç‡ä¸º0
            logger.debug(f"æ—¶é—´ {timestamp}: è¯¥æ—¶é—´æˆªé¢æ²¡æœ‰æ”¶ç›Šç‡æ•°æ®ï¼Œæ‰€æœ‰å“ç§æ”¶ç›Šç‡è®¾ä¸º0")
            return pd.Series(0.0, index=symbols)
            
        except Exception as e:
            logger.error(f"è·å–æ—¶é—´æˆªé¢ {timestamp} çš„æ”¶ç›Šç‡æ•°æ®å¤±è´¥: {e}")
            return pd.Series(0.0, index=symbols)

    def get_time_slice_data(self, pred_df: pd.DataFrame, timestamp: pd.Timestamp) -> pd.Series:
        """
        è·å–ç‰¹å®šæ—¶é—´æˆªé¢çš„é¢„æµ‹æ•°æ®
        
        Parameters
        ----------
        pred_df : pd.DataFrame
            å®Œæ•´é¢„æµ‹æ•°æ®
        timestamp : pd.Timestamp
            æ—¶é—´æˆ³
            
        Returns
        -------
        pd.Series
            è¯¥æ—¶é—´æˆªé¢çš„å› å­å¾—åˆ†
        """
        try:
            # è·å–è¯¥æ—¶é—´æˆªé¢çš„æ•°æ®
            time_slice = pred_df.loc[timestamp]
            factor_scores = time_slice['score']
            
            # æ—¥å¿—è¾“å‡º
            if len(factor_scores) > 0:
                logger.debug(f"æ—¶é—´ {timestamp}: è‚¡ç¥¨æ•°é‡={len(factor_scores)}, "
                           f"åˆ†æ•°èŒƒå›´=[{factor_scores.min():.4f}, {factor_scores.max():.4f}]")
            
            return factor_scores
        except KeyError:
            logger.warning(f"æ—¶é—´ {timestamp} æ— æ•°æ®")
            return pd.Series(dtype=float)
        except Exception as e:
            logger.error(f"è·å–æ—¶é—´æˆªé¢æ•°æ®å¤±è´¥ {timestamp}: {e}")
            return pd.Series(dtype=float)
    
    def calculate_portfolio_metrics(self, weights: pd.Series) -> Dict:
        """
        è®¡ç®—æŠ•èµ„ç»„åˆæŒ‡æ ‡
        
        Parameters
        ----------
        weights : pd.Series
            æƒé‡
            
        Returns
        -------
        Dict
            æŠ•èµ„ç»„åˆæŒ‡æ ‡
        """
        metrics = {}
        
        try:
            # åŸºæœ¬æŒ‡æ ‡
            metrics['total_weight'] = weights.sum()
            metrics['long_weight'] = weights[weights > 0].sum()
            metrics['short_weight'] = weights[weights < 0].sum()
            metrics['long_count'] = (weights > 0).sum()
            metrics['short_count'] = (weights < 0).sum()
            metrics['net_exposure'] = metrics['long_weight'] + metrics['short_weight']
            metrics['gross_exposure'] = weights.abs().sum()
            
            # çº¦æŸéªŒè¯
            config = self.optimizer.constraints_config
            # æ•°å€¼å®¹å·®å‚æ•° - ç”¨äºè§£å†³æµ®ç‚¹æ•°ç²¾åº¦è¯¯å·®
            tolerance = 1e-8  # 1e-8 çš„å®¹å·®è¶³å¤Ÿå¤„ç† IEEE 754 åŒç²¾åº¦æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
            norm_tolerance = 1e-6  # èŒƒæ•°çº¦æŸä½¿ç”¨ç¨å¤§çš„å®¹å·®ï¼Œå› ä¸ºèŒƒæ•°è®¡ç®—æ¶‰åŠç´¯ç§¯è¯¯å·®
            
            if 'weight_bounds' in config:
                min_w, max_w = config['weight_bounds']
                # æ·»åŠ å®¹å·®ï¼Œé¿å…å› æµ®ç‚¹æ•°ç²¾åº¦å¯¼è‡´çš„å¾®å°è¶…å‡º
                metrics['weight_in_bounds'] = (
                    (weights >= min_w - tolerance) & (weights <= max_w + tolerance)
                ).all()
            
            if 'norm_type' in config and 'norm_bound' in config:
                norm_type = config['norm_type']
                norm_bound = config['norm_bound']
                if norm_type == 'l1':
                    actual_norm = np.linalg.norm(weights, ord=1)
                    # æ·»åŠ å®¹å·®ï¼Œå…è®¸å¾®å°çš„èŒƒæ•°è¶…å‡ºï¼ˆç”±æ•°å€¼ç²¾åº¦è¯¯å·®å¯¼è‡´ï¼‰
                    metrics['norm_constraint_satisfied'] = actual_norm <= norm_bound + norm_tolerance
                    metrics['l1_norm'] = actual_norm
            
            if 'weight_sum' in config:
                expected_sum = config['weight_sum']
                # ä½¿ç”¨å®¹å·®å¤„ç†æƒé‡å’Œçš„å¾®å°è¯¯å·®
                metrics['weight_sum_error'] = abs(weights.sum() - expected_sum)
            
            
            return metrics
            
        except Exception as e:
            logger.error(f"è®¡ç®—æŠ•èµ„ç»„åˆæŒ‡æ ‡å¤±è´¥: {e}")
            return {}
    
    def calculate_turnover(self, new_weights: pd.Series) -> float:
        """
        è®¡ç®—æ¢æ‰‹ç‡
        
        Parameters
        ----------
        new_weights : pd.Series
            æ–°çš„æƒé‡
            
        Returns
        -------
        float
            æ¢æ‰‹ç‡
        """
        if self.current_weights is None:
            # ç¬¬ä¸€æ¬¡å¼€ä»“ï¼Œä¸è®¡å…¥æ¢æ‰‹ç‡ç»Ÿè®¡ï¼ˆè¿”å›0ï¼‰
            logger.debug(f"ç¬¬ä¸€æ¬¡å¼€ä»“ï¼Œä¸è®¡ç®—æ¢æ‰‹ç‡")
            return 0.0
        
        try:
            # å¯¹é½æƒé‡ - ä½¿ç”¨ä¸¤ä¸ªæƒé‡åˆ—è¡¨çš„å¹¶é›†ä½œä¸ºåŸºå‡†ï¼Œç¡®ä¿ä¸‹æ¶å“ç§ä¹Ÿè¢«è®¡ç®—
            all_symbols = self.current_weights.index.union(new_weights.index)
            
            # å¯¹é½å½“å‰æƒé‡åˆ°æ‰€æœ‰å“ç§ï¼Œä¸‹æ¶å“ç§ä¿ç•™å½“å‰æƒé‡
            aligned_current = self.current_weights.reindex(all_symbols, fill_value=0.0)
            # å¯¹é½æ–°æƒé‡åˆ°æ‰€æœ‰å“ç§ï¼Œæ–°å¢å“ç§æƒé‡ä¸º0
            aligned_new = new_weights.reindex(all_symbols, fill_value=0.0)
            
            # è®¡ç®—æ¢æ‰‹ç‡ï¼ˆä¸¤ä¸ªåºåˆ—çš„ç»å¯¹å·®å€¼ä¹‹å’Œï¼‰
            turnover = (aligned_new - aligned_current).abs().sum()
            
            logger.debug(f"æ¢æ‰‹ç‡è®¡ç®—: å½“å‰æƒé‡(å‰5ä¸ª): {aligned_current.head().values}, "
                        f"æ–°æƒé‡(å‰5ä¸ª): {aligned_new.head().values}, "
                        f"æƒé‡å·®å€¼(å‰5ä¸ª): {(aligned_new - aligned_current).head().values}, "
                        f"æ¢æ‰‹ç‡={turnover:.6f}")
            
            return turnover
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ¢æ‰‹ç‡å¤±è´¥: {e}")
            return 0.0
    
    def run_backtest(self, 
                    pred_df: pd.DataFrame,
                    rebalance_freq: str = 'hour',
                    start_date: Optional[str] = None,
                    end_date: Optional[str] = None,
                    verbose: bool = True,
                    max_periods: Optional[int] = None,
                    save_results: bool = True,
                    output_path: Optional[str] = None) -> Dict:
        """
        è¿è¡Œå›æµ‹ï¼šåœ¨æ¯ä¸ªæ—¶é—´æˆªé¢ï¼Œä½¿ç”¨ä¸Šä¸€æœŸæƒé‡è®¡ç®—å½“æœŸæ”¶ç›Šç‡ï¼Œç„¶åè°ƒä»“æ›´æ–°æƒé‡
        
        æ­£ç¡®çš„å›æµ‹é€»è¾‘ï¼š
        1. åœ¨tæ—¶åˆ»ï¼Œä½¿ç”¨t-1æ—¶åˆ»çš„æƒé‡è®¡ç®—t-1åˆ°tæœŸé—´çš„æ”¶ç›Šç‡
        2. æ”¶ç›Šç‡ = (tæ—¶åˆ»open - t-1æ—¶åˆ»open) / t-1æ—¶åˆ»open
        3. è®¡ç®—æ”¶ç›Šç‡åï¼Œæ‰§è¡Œè°ƒä»“ï¼Œæ›´æ–°æƒé‡ä¸ºtæ—¶åˆ»çš„æ–°æƒé‡
        4. æ–°æƒé‡ç”¨äºä¸‹ä¸€æœŸï¼ˆtåˆ°t+1æœŸé—´ï¼‰çš„æ”¶ç›Šè®¡ç®—
        
        Parameters
        ----------
        pred_df : pd.DataFrame
            é¢„æµ‹æ•°æ®ï¼ŒåŒ…å«å› å­å¾—åˆ†
        rebalance_freq : str
            è°ƒä»“é¢‘ç‡ï¼Œé»˜è®¤æ¯å°æ—¶
        start_date : str, optional
            å¼€å§‹æ—¥æœŸ
        end_date : str, optional
            ç»“æŸæ—¥æœŸ
        verbose : bool
            æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        max_periods : int, optional
            æœ€å¤§å›æµ‹æˆªé¢æ•°ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•ã€‚å¦‚æœä¸ºNoneåˆ™è¿è¡Œå…¨éƒ¨æˆªé¢
        save_results : bool, optional
            æ˜¯å¦ä¿å­˜å›æµ‹ç»“æœåˆ°æ–‡ä»¶
        output_path : str, optional
            å›æµ‹ç»“æœè¾“å‡ºè·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
            
        Returns
        -------
        Dict
            å›æµ‹ç»“æœç»Ÿè®¡
        """
        # è·å–æ—¶é—´åºåˆ—
        time_index = pred_df.index.get_level_values(0).unique()
        time_index = time_index.sort_values()
        
        # è¿‡æ»¤æ—¥æœŸèŒƒå›´
        if start_date:
            time_index = time_index[time_index >= pd.Timestamp(start_date)]
        if end_date:
            time_index = time_index[time_index <= pd.Timestamp(end_date)]
        
        # é™åˆ¶æœ€å¤§æˆªé¢æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        if max_periods is not None and len(time_index) > max_periods:
            time_index = time_index[:max_periods]
        
        # æ—¥å¿—è¾“å‡ºå›æµ‹åŸºæœ¬ä¿¡æ¯
        logger.info(f"å¼€å§‹è¿è¡Œå›æµ‹...")
        logger.info(f"è°ƒä»“é¢‘ç‡: {rebalance_freq}")
        logger.info(f"å›æµ‹æ—¶é—´èŒƒå›´: {time_index[0]} åˆ° {time_index[-1]}")
        logger.info(f"æ€»å…± {len(time_index)} ä¸ªæ—¶é—´æˆªé¢")
        
        # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if save_results and output_path is None:
            import datetime
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path(r"D:\temp\å›æµ‹è®°å½•")
            output_dir.mkdir(parents=True, exist_ok=True)
            # ç”Ÿæˆæ–‡ä»¶å
            current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = str(output_dir / f"backtest_results_{current_time}.parquet")
            logger.info(f"è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„: {output_path}")
        
        # é€ä¸ªæ—¶é—´æˆªé¢è¿›è¡Œä¼˜åŒ–
        successful_optimizations = 0
        failed_optimizations = 0
        
        # åˆå§‹åŒ–å†…å­˜ä¸­çš„ç»“æœå­˜å‚¨
        results_data = {
            'timestamp': [],
            'instrument': [],
            'current_weight': [],
            'previous_weight': [],
            'funding_rate': [],
            'factor_score': [],
            'individual_return': [],
            'factor_rank_pct': [],
            'portfolio_return': [],
            'capital': [],
            'turnover': [],
            'long_count': [],
            'short_count': [],
            'gross_exposure': [],
            'optimization_stage': [],
            'funding_cost': [],
            'commission_cost': []
        }
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå›æµ‹è¿›åº¦
        if HAS_TQDM:
            progress_bar = tqdm(total=len(time_index), desc="å›æµ‹è¿›åº¦", unit="æˆªé¢", 
                              bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]")
        else:
            print(f"å›æµ‹è¿›åº¦: 0/{len(time_index)}", end="")
        
        for i, timestamp in enumerate(time_index):
            status = ""
            try:
                # å¯¹äºä¼˜åŒ–å™¨ï¼Œä½¿ç”¨ä¸Šä¸€æœŸçš„æ•°æ®
                if i == 0:
                    # ç¬¬ä¸€ä¸ªæ—¶é—´æˆªé¢ï¼Œæ²¡æœ‰ä¸Šä¸€æœŸæ•°æ®ï¼ŒæŒ‰ç…§å®é™…åº”ç”¨å¤„ç†ï¼š
                    # 1. åˆå§‹åŒ–æƒé‡ä¸º0
                    # 2. è·³è¿‡ä¼˜åŒ–ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€ä¸ªæ—¶é—´æˆªé¢
                    logger.info(f"ç¬¬ä¸€ä¸ªæ—¶é—´æˆªé¢ {timestamp}ï¼Œæ²¡æœ‰ä¸Šä¸€æœŸæ•°æ®ï¼Œåˆå§‹åŒ–æƒé‡ä¸º0å¹¶è·³è¿‡ä¼˜åŒ–")
                    # åˆå§‹åŒ–æƒé‡ä¸º0
                    self.current_weights = pd.Series(0.0, index=pred_df.loc[timestamp].index) if len(pred_df.loc[timestamp]) > 0 else pd.Series(0.0)
                    # æ›´æ–°è¿›åº¦æ¡
                    if HAS_TQDM:
                        progress_bar.set_description(f"æˆªé¢ {i+1}/{len(time_index)} [åˆå§‹æƒé‡]")
                        progress_bar.update(1)
                    else:
                        print(f"\rå›æµ‹è¿›åº¦: {i+1}/{len(time_index)} [åˆå§‹æƒé‡]", end="")
                    # è·³è¿‡å½“å‰å¾ªç¯ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªæ—¶é—´æˆªé¢
                    continue
                else:
                    # è·å–ä¸Šä¸€æœŸæ—¶é—´æˆ³
                    prev_timestamp = time_index[i-1]
                    
                    # ä½¿ç”¨ä¸Šä¸€æœŸçš„å› å­å¾—åˆ†
                    factor_scores = self.get_time_slice_data(pred_df, prev_timestamp)
                    
                    if len(factor_scores) == 0:
                        status = "[æ— æ•°æ®]"
                        failed_optimizations += 1
                        # æ›´æ–°è¿›åº¦æ¡
                        if HAS_TQDM:
                            progress_bar.set_description(f"æˆªé¢ {i+1}/{len(time_index)} {status}")
                            progress_bar.update(1)
                        else:
                            print(f"\rå›æµ‹è¿›åº¦: {i+1}/{len(time_index)} {status}", end="")
                        continue
                    
                    # è®°å½•ä¸Šä¸€æœŸæƒé‡
                    previous_weights = self.current_weights.copy() if self.current_weights is not None else pd.Series(0.0, index=factor_scores.index)
                    
                    # è®¡ç®—å› å­å¾—åˆ†æ’åç™¾åˆ†æ¯”
                    # æ’åè¶Šé«˜ï¼Œç™¾åˆ†æ¯”è¶Šå¤§ï¼Œ1.0è¡¨ç¤ºæœ€é«˜ï¼Œ0.0è¡¨ç¤ºæœ€ä½
                    factor_rank = factor_scores.rank(method='dense', ascending=False)
                    total_assets = len(factor_scores)
                    factor_rank_pct = (total_assets - factor_rank + 1) / total_assets
                    
                    # å‡†å¤‡ä¸Šä¸€æœŸçš„èµ„é‡‘è´¹ç‡æ•°æ®
                    if self.funding_rates_data is not None:
                        # ä½¿ç”¨çœŸå®çš„èµ„é‡‘è´¹ç‡æ•°æ®
                        try:
                            funding_rates = get_funding_rates_for_timestamp(
                                self.funding_rates_data, prev_timestamp, factor_scores.index.tolist(), use_case='optimizer'
                            )
                        except Exception as e:
                            logger.error(f"è·å–ä¸Šä¸€æœŸèµ„é‡‘è´¹ç‡æ•°æ®å¤±è´¥ {prev_timestamp}: {e}")
                            status = "[èµ„é‡‘è´¹ç‡é”™è¯¯]"
                            failed_optimizations += 1
                            # æ›´æ–°è¿›åº¦æ¡
                            if HAS_TQDM:
                                progress_bar.set_description(f"æˆªé¢ {i+1}/{len(time_index)} {status}")
                                progress_bar.update(1)
                            else:
                                print(f"\rå›æµ‹è¿›åº¦: {i+1}/{len(time_index)} {status}", end="")
                            continue
                    else:
                        # ä½¿ç”¨é»˜è®¤çš„0èµ„é‡‘è´¹ç‡
                        funding_rates = pd.Series(0.0, index=factor_scores.index)
                
                # æ‰§è¡Œä¼˜åŒ–
                try:
                    optimized_weights, optimization_info = self.optimizer(
                        factor_scores=factor_scores,
                        funding_rates=funding_rates,
                        current_weights=self.current_weights
                    )
                    
                    if optimized_weights is not None and len(optimized_weights) > 0:
                        # è®¡ç®—æ¢æ‰‹ç‡
                        turnover = self.calculate_turnover(optimized_weights)
                        
                        # è®¡ç®—æŠ•èµ„ç»„åˆæŒ‡æ ‡
                        portfolio_metrics = self.calculate_portfolio_metrics(optimized_weights)
                        
                        # è®¡ç®—ç»„åˆæ”¶ç›Šç‡ï¼ˆå¦‚æœä»·æ ¼æ•°æ®å¯ç”¨ï¼‰- å¿…é¡»åœ¨æ›´æ–°æƒé‡ä¹‹å‰ï¼
                        portfolio_return = 0.0
                        if self.returns_data is not None:
                            # è®¡ç®—ç»„åˆæ”¶ç›Šç‡: w^T * rï¼Œä½¿ç”¨å½“å‰æœŸçš„æƒé‡
                            if optimized_weights is not None:
                                # è·å–å½“å‰æˆªé¢çš„æ”¶ç›Šç‡ï¼ˆç”¨äºè®¡ç®—å½“å‰æœŸçš„æ”¶ç›Šï¼‰
                                current_returns = self.get_returns_for_timestamp(timestamp, optimized_weights.index.tolist())
                                portfolio_return = (optimized_weights * current_returns).sum()
                                
                                # è®¡ç®—èµ„é‡‘è´¹ç‡æˆæœ¬å¹¶ä»ç»„åˆæ”¶ç›Šç‡ä¸­æ‰£é™¤
                                if self.funding_rates_data is not None:
                                    try:
                                        # è·å–å½“å‰æœŸçš„èµ„é‡‘è´¹ç‡æ•°æ®ï¼ˆä¸å½“å‰æœŸæƒé‡å¯¹é½ï¼‰
                                        funding_rates = get_funding_rates_for_timestamp(
                                            self.funding_rates_data, timestamp, optimized_weights.index.tolist(), use_case='portfolio'
                                        )
                                        # èµ„é‡‘è´¹ç‡æˆæœ¬ = å½“å‰æœŸæƒé‡ * å½“å‰æœŸèµ„é‡‘è´¹ç‡
                                        funding_cost = (optimized_weights * funding_rates).sum()
                                        # å°†èµ„é‡‘è´¹ç‡æˆæœ¬ä»ç»„åˆæ”¶ç›Šç‡ä¸­æ‰£é™¤
                                        portfolio_return += funding_cost
                                    except Exception as e:
                                        logger.warning(f"è·å–èµ„é‡‘è´¹ç‡æ•°æ®å¤±è´¥ {timestamp}: {e}")
                                        # èµ„é‡‘è´¹ç‡æˆæœ¬è®¡ç®—å¤±è´¥ï¼Œä¸å½±å“æ­£å¸¸å›æµ‹ï¼Œç»§ç»­æ‰§è¡Œ
                        
                        # è®¡ç®—æ‰‹ç»­è´¹æˆæœ¬ - åŸºäºæ¢æ‰‹ç‡ï¼Œå•è¾¹ä¸‡åˆ†ä¹‹äº”ï¼Œå¤šç©ºéƒ½æ”¶å–
                        # æ¢æ‰‹ç‡ä»£è¡¨äº†è°ƒä»“çš„æ¯”ä¾‹ï¼Œä¹Ÿå°±æ˜¯éœ€è¦æ”¶å–æ‰‹ç»­è´¹çš„éƒ¨åˆ†
                        commission_cost = turnover * self.commission_rate
                        # ä»ç»„åˆæ”¶ç›Šç‡ä¸­æ‰£é™¤æ‰‹ç»­è´¹
                        portfolio_return -= commission_cost
                        
                        # æ›´æ–°å½“å‰æƒé‡ä¸ºæ–°æƒé‡ - å¿…é¡»åœ¨è®¡ç®—æ”¶ç›Šç‡ä¹‹åï¼
                        self.current_weights = optimized_weights.copy()
                        
                        # æ›´æ–°èµ„é‡‘
                        self.current_capital *= (1 + portfolio_return)
                        
                        # è®°å½•å›æµ‹ç»“æœ
                        self.portfolio_history.append({
                            'timestamp': timestamp,
                            'weights': optimized_weights.copy(),
                            'metrics': portfolio_metrics,
                            'turnover': turnover,
                            'factor_scores': factor_scores.copy(),
                            'portfolio_return': portfolio_return,
                            'capital': self.current_capital
                        })
                        
                        # è®°å½•ä¼˜åŒ–çŠ¶æ€
                        self.optimization_status_history.append({
                            'timestamp': timestamp,
                            'attempt_count': 1,
                            'success': True,
                            'error_message': '',
                            'optimization_details': optimization_info
                        })
                        
                        # è·å–å½“å‰æˆªé¢çš„æ”¶ç›Šç‡æ•°æ®ï¼ˆç”¨äºè®°å½•æ¯ä¸ªå“ç§çš„å…·ä½“æ”¶ç›Šï¼‰
                        current_returns = pd.Series(0.0, index=optimized_weights.index)
                        if self.returns_data is not None:
                            # è·å–å½“å‰æˆªé¢çš„æ”¶ç›Šç‡
                            current_returns = self.get_returns_for_timestamp(timestamp, optimized_weights.index.tolist())
                        
                        # æ”¶é›†ç»“æœæ•°æ®åˆ°å†…å­˜ä¸­
                        optimization_stage = optimization_info.get('successful_stage', 'unknown') if optimization_info else 'unknown'
                        
                        # å¯¹é½previous_weightsåˆ°å½“å‰çš„å“ç§åˆ—è¡¨
                        aligned_previous_weights = previous_weights.reindex(optimized_weights.index, fill_value=0.0)
                        
                        # å¯¹é½factor_rank_pctåˆ°å½“å‰çš„å“ç§åˆ—è¡¨
                        aligned_factor_rank_pct = factor_rank_pct.reindex(optimized_weights.index, fill_value=0.0)
                        
                        # åˆå§‹åŒ–funding_costï¼Œå¦‚æœæ²¡æœ‰è®¡ç®—åˆ™ä¸º0
                        funding_cost_value = funding_cost if 'funding_cost' in locals() else 0.0
                        
                        for instrument in optimized_weights.index:
                            results_data['timestamp'].append(timestamp)
                            results_data['instrument'].append(instrument)
                            results_data['current_weight'].append(optimized_weights[instrument])
                            results_data['previous_weight'].append(aligned_previous_weights[instrument])
                            results_data['funding_rate'].append(funding_rates[instrument])
                            results_data['factor_score'].append(factor_scores[instrument])
                            results_data['individual_return'].append(current_returns[instrument])
                            results_data['factor_rank_pct'].append(aligned_factor_rank_pct[instrument])
                            results_data['portfolio_return'].append(portfolio_return)
                            results_data['capital'].append(self.current_capital)
                            results_data['turnover'].append(turnover)
                            results_data['long_count'].append(portfolio_metrics.get('long_count', 0))
                            results_data['short_count'].append(portfolio_metrics.get('short_count', 0))
                            results_data['gross_exposure'].append(portfolio_metrics.get('gross_exposure', 0))
                            results_data['optimization_stage'].append(optimization_stage)
                            results_data['funding_cost'].append(funding_cost_value)
                            results_data['commission_cost'].append(commission_cost)
                        
                        successful_optimizations += 1
                        status = "[æˆåŠŸ]"
                    else:
                        # è®°å½•ä¼˜åŒ–å¤±è´¥çŠ¶æ€
                        self.optimization_status_history.append({
                            'timestamp': timestamp,
                            'attempt_count': 1,
                            'success': False,
                            'error_message': 'Optimization failed to converge',
                            'optimization_details': None
                        })
                        status = "[ä¼˜åŒ–å¤±è´¥]"
                        failed_optimizations += 1
                
                except Exception as opt_error:
                    # è®°å½•ä¼˜åŒ–é”™è¯¯çŠ¶æ€
                    self.optimization_status_history.append({
                        'timestamp': timestamp,
                        'attempt_count': 1,
                        'success': False,
                        'error_message': str(opt_error),
                        'optimization_details': None
                    })
                    status = "[é”™è¯¯]"
                    failed_optimizations += 1
            
            except Exception as e:
                status = "[å¤„ç†é”™è¯¯]"
                failed_optimizations += 1
            
            # ç»Ÿä¸€æ›´æ–°è¿›åº¦æ¡ï¼ˆåªæ›´æ–°ä¸€æ¬¡ï¼‰
            if HAS_TQDM:
                progress_bar.set_description(f"æˆªé¢ {i+1}/{len(time_index)} {status}")
                progress_bar.update(1)
            else:
                print(f"\rå›æµ‹è¿›åº¦: {i+1}/{len(time_index)} {status}", end="")
        
        # å…³é—­è¿›åº¦æ¡
        if HAS_TQDM:
            progress_bar.close()
        else:
            print()  # æ¢è¡Œ
        
        # è®¡ç®—å›æµ‹ç»Ÿè®¡ç»“æœ
        backtest_stats = self._calculate_backtest_statistics(rebalance_freq)
        
        logger.info(f"\n=== å›æµ‹å®Œæˆ ===")
        logger.info(f"æˆåŠŸä¼˜åŒ–: {successful_optimizations}")
        logger.info(f"å¤±è´¥ä¼˜åŒ–: {failed_optimizations}")
        logger.info(f"æ€»æˆªé¢æ•°: {len(time_index)}")
        
        # é¿å…é™¤é›¶é”™è¯¯
        total_attempts = successful_optimizations + failed_optimizations
        if total_attempts > 0:
            success_rate = successful_optimizations / total_attempts * 100
            logger.info(f"æˆåŠŸç‡: {success_rate:.2f}%")
        else:
            logger.warning("æ²¡æœ‰æˆåŠŸçš„ä¼˜åŒ–å°è¯•")
            success_rate = 0.0
        
        # å¦‚æœéœ€è¦ï¼Œä¸€æ¬¡æ€§ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        if save_results and output_path:
            self._save_results_to_file(results_data, output_path)
        
        return backtest_stats
    
    def _log_portfolio_details(self, 
                              iteration: int, 
                              timestamp: pd.Timestamp, 
                              weights: pd.Series, 
                              metrics: Dict, 
                              turnover: float):
        """è®°å½•æŠ•èµ„ç»„åˆè¯¦ç»†ä¿¡æ¯"""
        logger.info(f"è¿­ä»£ {iteration}: æŠ•èµ„ç»„åˆè¯¦æƒ…")
        logger.info(f"  æ—¶é—´: {timestamp}")
        logger.info(f"  è‚¡ç¥¨æ•°é‡: {len(weights)}")
        logger.info(f"  å¤šå¤´æ•°é‡: {metrics.get('long_count', 0)}, ç©ºå¤´æ•°é‡: {metrics.get('short_count', 0)}")
        logger.info(f"  å¤šå¤´æƒé‡: {metrics.get('long_weight', 0):.4f}, ç©ºå¤´æƒé‡: {metrics.get('short_weight', 0):.4f}")
        logger.info(f"  å‡€æ•å£: {metrics.get('net_exposure', 0):.4f}, æ€»æ•å£: {metrics.get('gross_exposure', 0):.4f}")
        logger.info(f"  æ¢æ‰‹ç‡: {turnover:.4f}")
        
        if 'l1_norm' in metrics:
            logger.info(f"  L1èŒƒæ•°: {metrics['l1_norm']:.4f}")
        
        if 'weight_in_bounds' in metrics:
            status = "âœ…" if metrics['weight_in_bounds'] else "âŒ"
            logger.info(f"  æƒé‡è¾¹ç•Œçº¦æŸ: {status}")
    
    def _save_results_to_file(self, results_data: Dict, output_path: str) -> None:
        """
        å°†å†…å­˜ä¸­çš„å›æµ‹ç»“æœä¸€æ¬¡æ€§ä¿å­˜åˆ°æ–‡ä»¶
        
        Parameters
        ----------
        results_data : Dict
            å†…å­˜ä¸­å­˜å‚¨çš„å›æµ‹ç»“æœæ•°æ®
        output_path : str
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not results_data or not results_data['timestamp']:
            logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        try:
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(results_data)
            
            # è®¾ç½®MultiIndex
            df.set_index(['timestamp', 'instrument'], inplace=True)
            
            # ä¿å­˜åˆ°parquetæ–‡ä»¶
            df.to_parquet(output_path, engine='pyarrow', compression='snappy')
            
            logger.info(f"å›æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
            logger.info(f"æ—¶é—´èŒƒå›´: {df.index.get_level_values(0).min()} åˆ° {df.index.get_level_values(0).max()}")
            logger.info(f"å“ç§æ•°é‡: {df.index.get_level_values(1).nunique()}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å›æµ‹ç»“æœå¤±è´¥: {e}")
            raise


    

    
    def _calculate_backtest_statistics(self, rebalance_freq: str) -> Dict:
        """è®¡ç®—å›æµ‹ç»Ÿè®¡ç»“æœ"""
        if not self.portfolio_history:
            return {}
        
        stats = {}
        
        try:
            # æå–æŒ‡æ ‡
            turnovers = [record['turnover'] for record in self.portfolio_history]
            long_counts = [record['metrics'].get('long_count', 0) for record in self.portfolio_history]
            short_counts = [record['metrics'].get('short_count', 0) for record in self.portfolio_history]
            portfolio_returns = [record.get('portfolio_return', 0.0) for record in self.portfolio_history]
            capitals = [record.get('capital', self.initial_capital) for record in self.portfolio_history]
            short_counts = [record['metrics'].get('short_count', 0) for record in self.portfolio_history]
            gross_exposures = [record['metrics'].get('gross_exposure', 0) for record in self.portfolio_history]
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats['total_rebalances'] = len(self.portfolio_history)
            stats['avg_turnover'] = np.mean(turnovers)
            stats['max_turnover'] = np.max(turnovers)
            stats['min_turnover'] = np.min(turnovers)
            stats['avg_long_count'] = np.mean(long_counts)
            stats['avg_short_count'] = np.mean(short_counts)
            stats['avg_gross_exposure'] = np.mean(gross_exposures)
            
            # æ”¶ç›Šç»Ÿè®¡ï¼ˆå¦‚æœä»·æ ¼æ•°æ®å¯ç”¨ï¼‰
            if portfolio_returns:
                stats['total_return'] = (capitals[-1] - self.initial_capital) / self.initial_capital
                stats['avg_portfolio_return'] = np.mean(portfolio_returns)
                stats['std_portfolio_return'] = np.std(portfolio_returns)
                stats['max_portfolio_return'] = np.max(portfolio_returns)
                stats['min_portfolio_return'] = np.min(portfolio_returns)
                
                # æ ¹æ®è°ƒä»“é¢‘ç‡è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡å’Œå¤æ™®æ¯”ç‡ï¼ˆè€ƒè™‘åŠ å¯†è´§å¸å¸‚åœº7*24å°æ—¶äº¤æ˜“ç‰¹ç‚¹ï¼‰
                if rebalance_freq == 'daily':
                    annualization_factor = 365  # åŠ å¯†è´§å¸å¸‚åœº7*24å°æ—¶ï¼Œæ¯å¹´365å¤©
                elif rebalance_freq == 'weekly':
                    annualization_factor = 365 / 7  # åŠ å¯†è´§å¸å¸‚åœº7*24å°æ—¶ï¼Œæ¯å¹´çº¦52.14å‘¨
                elif rebalance_freq == 'monthly':
                    annualization_factor = 365 / 30  # åŠ å¯†è´§å¸å¸‚åœº7*24å°æ—¶ï¼Œæ¯å¹´çº¦12.17æœˆ
                elif rebalance_freq == 'hour':
                    annualization_factor = 365 * 24  # åŠ å¯†è´§å¸å¸‚åœº7*24å°æ—¶ï¼Œæ¯å¹´8760å°æ—¶
                else:
                    annualization_factor = 365 * 24  # é»˜è®¤æŒ‰å°æ—¶è®¡ç®—ï¼Œæ¯å¹´8760å°æ—¶
                
                # è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡
                stats['annualized_return'] = stats['avg_portfolio_return'] * annualization_factor
                # è®¡ç®—å¹´åŒ–æ³¢åŠ¨ç‡
                stats['annualized_volatility'] = stats['std_portfolio_return'] * np.sqrt(annualization_factor)
                
                # è®¡ç®—å¹´åŒ–å¤æ™®æ¯”ç‡ï¼ˆè€ƒè™‘æ— é£é™©åˆ©ç‡ï¼Œé»˜è®¤æ— é£é™©åˆ©ç‡ä¸º0ï¼‰
                risk_free_rate = 0.0  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ— é£é™©åˆ©ç‡
                stats['sharpe_ratio'] = (stats['annualized_return'] - risk_free_rate) / stats['annualized_volatility'] if stats['annualized_volatility'] > 0 else 0
                
                # è®¡ç®—æœ€å¤§å›æ’¤
                cumulative_returns = np.cumprod(1 + np.array(portfolio_returns))
                running_max = np.maximum.accumulate(cumulative_returns)
                drawdowns = (cumulative_returns - running_max) / running_max
                stats['max_drawdown'] = np.min(drawdowns) if len(drawdowns) > 0 else 0
                
                # èƒœç‡ï¼ˆæ­£æ”¶ç›Šçš„æ¯”ä¾‹ï¼‰
                positive_returns = [r for r in portfolio_returns if r > 0]
                stats['win_rate'] = len(positive_returns) / len(portfolio_returns) if portfolio_returns else 0
            
            # çº¦æŸæ»¡è¶³ç‡
            weight_bounds_violations = 0
            norm_violations = 0
            weight_sum_violations = 0
            
            # ä½¿ç”¨ä¸éªŒè¯ç›¸åŒçš„å®¹å·®å‚æ•°
            tolerance = 1e-8
            norm_tolerance = 1e-6  # èŒƒæ•°çº¦æŸä½¿ç”¨ä¸calculate_portfolio_metricsç›¸åŒçš„å®¹å·®
            
            for record in self.portfolio_history:
                metrics = record['metrics']
                if not metrics.get('weight_in_bounds', True):
                    weight_bounds_violations += 1
                if not metrics.get('norm_constraint_satisfied', True):
                    norm_violations += 1
                # æƒé‡å’Œçº¦æŸçš„å®¹å·®è®¾ç½®å¾—æ›´å®½æ¾ä¸€äº›ï¼Œå› ä¸ºè¿™æ˜¯ä¼˜åŒ–ç›®æ ‡
                if metrics.get('weight_sum_error', 0) > tolerance * 10:
                    weight_sum_violations += 1
            
            total_records = len(self.portfolio_history)
            stats['weight_bounds_violation_rate'] = weight_bounds_violations / total_records
            stats['norm_violation_rate'] = norm_violations / total_records
            stats['weight_sum_violation_rate'] = weight_sum_violations / total_records
            
            logger.info(f"\n=== å›æµ‹ç»Ÿè®¡ç»“æœ ===")
            logger.info(f"æ€»è°ƒä»“æ¬¡æ•°: {stats['total_rebalances']}")
            logger.info(f"å¹³å‡æ¢æ‰‹ç‡: {stats['avg_turnover']:.4f}")
            logger.info(f"æœ€å¤§æ¢æ‰‹ç‡: {stats['max_turnover']:.4f}")
            logger.info(f"å¹³å‡å¤šå¤´æ•°é‡: {stats['avg_long_count']:.1f}")
            logger.info(f"å¹³å‡ç©ºå¤´æ•°é‡: {stats['avg_short_count']:.1f}")
            logger.info(f"å¹³å‡æ€»æ•å£: {stats['avg_gross_exposure']:.4f}")
            logger.info(f"æƒé‡è¾¹ç•Œçº¦æŸè¿åç‡: {stats['weight_bounds_violation_rate']:.2%}")
            logger.info(f"èŒƒæ•°çº¦æŸè¿åç‡: {stats['norm_violation_rate']:.2%}")
            logger.info(f"æƒé‡å’Œçº¦æŸè¿åç‡: {stats['weight_sum_violation_rate']:.2%}")
            
            # è¾“å‡ºæ”¶ç›Šç»Ÿè®¡ï¼ˆå¦‚æœä»·æ ¼æ•°æ®å¯ç”¨ï¼‰
            if portfolio_returns:
                logger.info(f"\n=== æ”¶ç›Šç»Ÿè®¡ ===")
                logger.info(f"æ€»æ”¶ç›Šç‡: {stats['total_return']:.4f} ({stats['total_return']*100:.2f}%)")
                logger.info(f"å¹³å‡ç»„åˆæ”¶ç›Šç‡: {stats['avg_portfolio_return']:.6f}")
                logger.info(f"ç»„åˆæ”¶ç›Šç‡æ ‡å‡†å·®: {stats['std_portfolio_return']:.6f}")
                logger.info(f"æœ€å¤§å•æœŸæ”¶ç›Š: {stats['max_portfolio_return']:.6f}")
                logger.info(f"æœ€å°å•æœŸæ”¶ç›Š: {stats['min_portfolio_return']:.6f}")
                logger.info(f"å¤æ™®æ¯”ç‡: {stats['sharpe_ratio']:.4f}")
                logger.info(f"æœ€å¤§å›æ’¤: {stats['max_drawdown']:.4f} ({stats['max_drawdown']*100:.2f}%)")
                logger.info(f"èƒœç‡: {stats['win_rate']:.4f} ({stats['win_rate']*100:.2f}%)")
            
            return stats
            
        except Exception as e:
            logger.error(f"è®¡ç®—å›æµ‹ç»Ÿè®¡å¤±è´¥: {e}")
            return {}


def constrained_optimizer_backtest(max_periods: int = None):
    """æµ‹è¯•ConstrainedPortfolioOptimizerçš„å›æµ‹åŠŸèƒ½
    
    Args:
        max_periods: æœ€å¤§å›æµ‹æˆªé¢æ•°ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•ã€‚å¦‚æœä¸ºNoneåˆ™è¿è¡Œå…¨éƒ¨æ•°æ®
    """
    logger.info("=== å¼€å§‹æµ‹è¯•ConstrainedPortfolioOptimizerå›æµ‹åŠŸèƒ½ ===")
    
    # æ•°æ®è·¯å¾„
    data_path = r"D:\PycharmProjects\qlib\workspace\notebooks\filtered_pred_df.parquet"
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_path):
        logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        logger.info("åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        np.random.seed(42)
        dates = pd.date_range('2023-06-14', periods=10, freq='1H')  # 10ä¸ªå°æ—¶ç”¨äºæµ‹è¯•
        instruments = [f"{i:04d}USDT" for i in range(20)]  # 20ä¸ªå“ç§
        
        index_tuples = []
        values = []
        
        for date in dates:
            for instrument in instruments:
                index_tuples.append((date, instrument))
                values.append(np.random.randn())
        
        pred_df = pd.DataFrame(
            {'score': values},
            index=pd.MultiIndex.from_tuples(index_tuples, names=['datetime', 'instrument'])
        )
        
        logger.info(f"æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆï¼Œå½¢çŠ¶: {pred_df.shape}")
        
    else:
        logger.info("ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œæµ‹è¯•")
        try:
            pred_df = pd.read_parquet(data_path)
            # ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œå›æµ‹
            logger.info(f"çœŸå®æ•°æ®åŠ è½½å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {pred_df.shape}")
            logger.info(f"æ—¶é—´èŒƒå›´: {pred_df.index.get_level_values(0).min()} åˆ° {pred_df.index.get_level_values(0).max()}")
            logger.info(f"è‚¡ç¥¨æ•°é‡: {pred_df.index.get_level_values(1).nunique()}")
        except Exception as e:
            logger.error(f"åŠ è½½çœŸå®æ•°æ®å¤±è´¥: {e}")
            return
    
    # é…ç½®çº¦æŸæ¡ä»¶ - æˆªé¢ä¸­æ€§ç­–ç•¥
    constraints_config = {
        'weight_bounds': (-0.01, 0.01),      # æƒé‡è¾¹ç•Œï¼š-1% åˆ° +1%
        # 'weight_bounds': (-0.05, 0.05),      # æƒé‡è¾¹ç•Œï¼š-5% åˆ° +5%
        'weight_sum': 0.0,                 # æƒé‡å’Œ = 0ï¼ˆæˆªé¢ä¸­æ€§ï¼‰
        'norm_type': 'l1',                 # L1èŒƒæ•°çº¦æŸ
        'norm_bound': 1.0,                 # ||w||_1 â‰¤ 1ï¼Œæ§åˆ¶æ€»æ æ†ç‡
        'turnover': 0.01,                   # æ¢æ‰‹ç‡çº¦æŸ 10%
        # 'turnover': 0.2,                   # æ¢æ‰‹ç‡çº¦æŸ 10%
        # 'turnover': 1,                   # ä¸åšæ¢æ‰‹ç‡çº¦æŸ
    }
    
    logger.info(f"çº¦æŸé…ç½®: {constraints_config}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆæ·»åŠ æƒé‡é˜ˆå€¼è¿‡æ»¤ï¼‰
    optimizer = ConstrainedPortfolioOptimizer(
        constraints_config=constraints_config,
        objective_type="max_factor_score",
        solver=None,  # ä½¿ç”¨é»˜è®¤æ±‚è§£å™¨
        solver_kwargs={'verbose': False},
        weight_threshold=0.001,  # è¿‡æ»¤ç»å¯¹å€¼å°äº0.001çš„æƒé‡ï¼ˆè¿›ä¸€æ­¥æ”¾å®½é˜ˆå€¼ï¼‰
        enable_funding_rate_constraint=True # å¯ç”¨èµ„é‡‘è´¹ç‡çº¦æŸ
    )
    
    logger.info("ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆ")
    
    # åŠ è½½èµ„é‡‘è´¹ç‡æ•°æ®
    funding_rates_data = load_funding_rates_data()
    
    # åˆ›å»ºå›æµ‹æ¨¡æ‹Ÿå™¨
    price_data = pd.read_parquet(r"D:\temp\å›æµ‹ç”¨çš„å“ç§close_open\open_close.parquet")
    
    backtest_simulator = BacktestSimulator(
        optimizer=optimizer,
        initial_capital=1000000.0,
        funding_rates_data=funding_rates_data,
        price_data=price_data
    )
    
    # è¿è¡Œå…¨é‡å›æµ‹
    try:
        # è·å–æ‰€æœ‰æ—¶é—´æˆªé¢
        time_index = pred_df.index.get_level_values(0).unique()
        time_index = time_index.sort_values()
        
        logger.info(f"å…¨é‡å›æµ‹æ•°æ®ç»Ÿè®¡:")
        logger.info(f"  æ€»æ—¶é—´æˆªé¢æ•°: {len(time_index)}")
        logger.info(f"  æ—¶é—´èŒƒå›´: {time_index.min()} åˆ° {time_index.max()}")
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦é™åˆ¶å›æµ‹æˆªé¢æ•°
        if max_periods is not None:
            logger.info(f"  âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼: åªè¿è¡Œå‰{max_periods}ä¸ªæˆªé¢")
        else:
            logger.info(f"  ğŸ“Š å®Œæ•´å›æµ‹æ¨¡å¼: è¿è¡Œå…¨éƒ¨{len(time_index)}ä¸ªæˆªé¢")
        
        backtest_stats = backtest_simulator.run_backtest(
            pred_df=pred_df,
            rebalance_freq='hour',
            verbose=True,
            max_periods=max_periods
        )
        
        # åªæ˜¾ç¤ºæœ€ç»ˆç»“æœ
        logger.info("âœ… å›æµ‹æµ‹è¯•å®Œæˆ")
        logger.info(f"å›æµ‹ç»Ÿè®¡: æˆåŠŸç‡100%, å¹³å‡æ¢æ‰‹ç‡{backtest_stats.get('avg_turnover', 0):.4f}")
        
        # å¦‚æœæœ‰æ”¶ç›Šç»Ÿè®¡ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if 'total_return' in backtest_stats:
            logger.info(f"\nğŸ“Š æ”¶ç›Šè¡¨ç°:")
            logger.info(f"  æ€»æ”¶ç›Šç‡: {backtest_stats['total_return']*100:.2f}%")
            logger.info(f"  å¹´åŒ–æ”¶ç›Šç‡: {backtest_stats['annualized_return']*100:.2f}%")
            logger.info(f"  å¤æ™®æ¯”ç‡: {backtest_stats['sharpe_ratio']:.4f}")
            logger.info(f"  æœ€å¤§å›æ’¤: {backtest_stats['max_drawdown']*100:.2f}%")
            logger.info(f"  èƒœç‡: {backtest_stats['win_rate']*100:.2f}%")
            logger.info(f"  å¹³å‡å•æœŸæ”¶ç›Š: {backtest_stats['avg_portfolio_return']*100:.4f}%")
        
        return backtest_stats
        
    except Exception as e:
        logger.error(f"å›æµ‹å¤±è´¥: {e}")
        raise





def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æ§åˆ¶"""
    import argparse
    
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='ConstrainedPortfolioOptimizerå›æµ‹æµ‹è¯•')
    parser.add_argument('--max-periods', type=int, default=None,
                       help='æœ€å¤§å›æµ‹æˆªé¢æ•°ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•ã€‚å¦‚æœä¸æŒ‡å®šåˆ™è¿è¡Œå…¨éƒ¨æ•°æ®')
    parser.add_argument('--quick-test', action='store_true',
                       help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼Œåªè¿è¡Œå‰100ä¸ªæˆªé¢')
    parser.add_argument('--full-test', action='store_true',
                       help='å®Œæ•´æµ‹è¯•æ¨¡å¼ï¼Œè¿è¡Œå…¨éƒ¨æ•°æ®ï¼ˆé»˜è®¤ï¼‰')
    
    args = parser.parse_args()
    
    # ç¡®å®šmax_periodså‚æ•°
    if args.quick_test:
        max_periods = 100
        logger.info("ğŸš€ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªè¿è¡Œå‰100ä¸ªæˆªé¢")
    elif args.max_periods is not None:
        max_periods = args.max_periods
        logger.info(f"ğŸ¯ è‡ªå®šä¹‰æµ‹è¯•æ¨¡å¼ï¼šè¿è¡Œå‰{max_periods}ä¸ªæˆªé¢")
    else:
        max_periods = None
        logger.info("ğŸ“Š å®Œæ•´æµ‹è¯•æ¨¡å¼ï¼šè¿è¡Œå…¨éƒ¨æ•°æ®")
    
    logger.info("å¼€å§‹è¿è¡ŒConstrainedPortfolioOptimizerç»¼åˆæµ‹è¯•")
    
    # æŠ‘åˆ¶è­¦å‘Š
    warnings.filterwarnings('ignore')
    
    # å¼€å¯DEBUGæ—¥å¿—ä»¥æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
    logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # åªè¿è¡Œå›æµ‹åŠŸèƒ½æµ‹è¯•ï¼ˆè·³è¿‡å…¶ä»–æµ‹è¯•ï¼‰
        logger.info("\n=== è¿è¡Œå›æµ‹åŠŸèƒ½æµ‹è¯• ===")
        constrained_optimizer_backtest(max_periods=max_periods)
        
        logger.info("\nğŸ‰ å›æµ‹æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()
